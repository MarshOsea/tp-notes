## 2020-02-10 
#### 开发高性能消息引擎的秘诀：Apache RocketMQ  冯嘉，阿里巴巴高级技术专家
1. 作为一款消息中间件，RocketMQ需要解决的技术问题其实在分布式系统领域中都有所体现
。分布式系统的领域按照分布式通信、分布式存储、分布式计算以及分布式管理这四大部分进行划分，其实就会发现在这四大技术栈下有很多的子技术。
**分布式通信**：通信协议，如TCP、HTTP。I/O模型的选择，如：Boss/Worker模型、Only Boss模型
**分布式存储**：分布式存储最复杂，包括结构化存储、半结构化村村、非结构化存储等
**分布式计算**：Streaming计算、图计算等
**分布式管理**：牵扯到数据多副本的问题，还有分布式一致性、协调等

2. RocketMQ整体架构
RocketMQ主要分成四大部分：name server、brokers、producers、consumers。
**NameServer**：NameServer提供轻量级的服务发现和路由功能。
**Brokers**：Broker通过Topic和Queue机制提供消息的存储服务，支持推拉模式，并支持容错机制（多副本）
**Producer**: Producer从NameServer获取到Brokers列表，然后通过客户端负载均衡的方式调用Brokers，发送过程支持快速失败和低延时
**Consumer**：Consumer支持推拉两种模式

3. Broker内部的架构
Broker作为RocketMQ中最重要的部分，负责消息存储、派发、查询、HA保证等等。Broker内部有以下部分组成。
**Remoting Module：** broker的入口，处理客户端请求
**Client Manager：** 管理客户端（生产者/消费者）和消费者与订阅主题的对应关系
**Store Server：** 存储API层
**HA Server：** 高可用，处理master broker与slave broker之间的数据同步
**Index Server：** 为消息构建索引，并提供消息的快速查询服务

其实视频中并没有真正的强调RocketMQ为什么高性能， 因为RocketMQ前期是参考kafka的，所以可以参考kafka的高性能原因：顺序写、sendfile、传输压缩、mmap，另外RocketMQ在tcp层面、pagecache jvm选择g1等层面都做了特别的优化。其实个人认为更多的优势在于RocketMQ相比kafka多了一些特性，如：XA协议等。

## 2020-02-11
#### 全球跨域RPC架构 高良 菜鸟网络互联平台技术主管
菜鸟物流已经实现了全球买、全球卖。对一个国际物流来说，可能需要对接全球多地的合作商，而中心化部署的模式暴露出了很多缺点：延迟高（7S）、失败率高（40%）、稳定性低、难以支持海外本地多样化的业务模式。所以提出了去中心化的架构模式，如下图所示：
1. 服务注册中心：每个区域都有自己的服务注册中心，consumer从自己的服务注册中心发现provider
2. GN：global network是一个区域内的网关，GN会把自己注册到服务注册中心中，不同区域的GN之间是通过专线联通，并且互相通信发现。
3. consumer跨域调用时，就把请求转发给GN，由GN转发给其他区域的GN，其他区域的GN再把请求转发给正确的provider

#### 大道至简 - 严选售后服务架构演变实践 马超
其实视频中分享的架构演变模式很常见，即从单体应用->引入rpc、配置中心、少部分微服务->引入分布式事务、分布式锁、消息之后产生大量微服务的演进过程。个人认为微服务化并不能提升∵开发效率的原因主要有两个：
1. 研发对于模型的抽象理解与业务人员不一致，这样微服务化之后随着业务的变化越来越多可能会适得其反。
2. 微服务化之后所引入的分布式技术组件研发不成熟，如：分布式事务、分布式锁、分布式消息系统、缓存、搜索引擎，分布式日志、trace、分布式配置、无状态华改造等。

#### SkyWalking的发展之路--从无名小卒到拥抱全球
作者讲的其实很接地气，Skywalking最开始的代码也是很low的，为什么最终能够成为一个Apache的顶级项目，个人认为有以下几点：
1. 不要试图从一开始设计好所有的架构点，看起来复杂的架构是根据需求、业务驱动演化出来的
2. 开源项目是一个持续性的过程，尝试让一些小公司开始使用，尝试让别人知道自己这个项目在做什么
3. 项目部署一定要简单，对外部的依赖尽可能的少
4. 注重文档

#### 如何利用混沌工程打造健壮的分布式系统 唐刘 PingCAP
1. 为什么需要Chaos
    1.1. 分布式服务场景下，在服务器很多的情况下，单台少少部分服务器断电，进程crash，磁盘损坏等场景发生的概率大大增加。
    1.2. 常规的单元测试、集成测试、性能测试难以覆盖线上各种意外的场景，如：Linux磁盘满了、服务器断电了、CPU满了等等

2. Chaos的操作步骤：
    2.1. 定义稳定状态，如QPS、成功率、响应时间等Metrics指标
    2.2. 引入现实环境中的变量，如：断电、进程crash、网络断开等。另外需要在现实环境/生产环境做测试，还要控制爆炸半径。
    2.3. 验证引入变量之后的状态是否可以接受。在变量引入的时候，只要状态没有太大的抖动就认为是正常的。

3. Chaos要实现自动化，通过模拟、拦截（引入fuse等项目）等方式来引入变量

## 2020-02-12
#### 微众银行核心数据库架构演进 胡盼盼
微众银行的数据库架构演进过程：
1. 架构1.0：基于腾讯的TD-SQL，基于两地三中心的架构，在每个中心部署一套SET（一个SET包含1个master、2个slave，SET内部是基于TD-SQL实现的强一致）。中心之间是通过异步的方式同步数据，是最终一致的，所以此时只能有一个中心对外提供服务，服务器资源浪费较为严重，也不够高可用。
2. 架构2.0：同城多活。同城部署多个IDC，同城的IDC之间的数据是强一致的，所以同城所有IDC都是可以对外提供服务，大大提高了服务器的资源利用率。缺点是会有性能损失，解决方案是：1. 控制同城IDC之间的距离在50公里内，2. IDC之间建立多条专线，3. 服务器万兆网络。最终效果把跨机房同步的性能影响在10%以内
3. NEWSQL的应用，微众银行通过对比现有NEWSQL产品，基于开源和一些功能特性要求最终选择了TiDB，TiDB在微众银行的部署架构也是在同城多IDC之间部署集群，保证多IDC之间的数据是强一致性的。
4. 异地多活，异地多活方案也是在研讨之中，异地多活会面临很多问题：多点写入的冲突与同步、跨城数据传输、跨城数据一致性保证等。其实这个方面可以参考蚂蚁金服的单元化部署方案，每一个IDC都部署全量的应用服务、根据用户ID sharding到固定的IDC处理用户请求，来减少冲突的发生。

#### NewSQL在转转的千亿规模应用实践 孙玄
1. 为什么要使用NewSQL
    1. 分表分表方案在做一些其他维度查询时对业务侵入比较大
    2. 第一次分库分表之后表又不够用了，需要多次分库分表，数据迁移比较麻烦
    3. 大表的DDL操作耗时很长，对运维不友好
2. TiDB的特点
    1. 开源分布式NewSQL关系型数据库
    2. 自动水平伸缩
    3. 强一致性的分布式事务
    4. 基于Raft算法的多副本复制
    5. 高度兼容MySQl协议
3. TiDB使用之后带来的变化
    1. 业务请求队列数：MySQL时随着流量的变化抖动较大，TiDB恒为1（没有业务请求等待）
    2. 业务延时和错误量：MySQL耗时有抖动，而TiDB很稳定。MySql会有错误请求（错误原因是MySql处理不及时超时丢弃），而TiDB错误量几乎为0
 
#### 支撑亿级运单的配运平台架构实践 赵玉开 京东物流架构师
如何应对亿级运单：
1. 存储架构演进
其实作者分享的应用架构以及存储架构的演进都是常规的从单体应用专项微服务架构的演进过程，分库分表、微服务化拆分等等。
2. 业务的抽象
    后半段在讲一些基于京东物流的业务如何做系统上的抽象、分治。

#### 高可用分布式流数据存储设计 李玥 京东集团中台技术架构部资深架构师
作者介绍了京东的一个分布式流数据存储产品JournalQ。JournalQ是一个类似于kafka的分布式消息存储产品，包括底层的存储架构设计都是与kafka类似的，通过一些细节上的优化达到比kafka性能更好的效果。
1. 存储结构：通过index和消息分开存储的方式，实现读写都是O(1)操作。
2. 堆外内存：kafka也是使用堆外内存，减少Java的GC
3. 异步预加载：当Consumer在读一条消息时，异步的把下一条消息提前加载到内存中
4. 读写共页：主要是在写操作时先把数据放入内存中，因为此时有较大的概率数据会被读到
5. 使用raft协议实现多副本复制
6. 数据的整个流转过程使用异步的方式提升并发能力

#### 百万级并发商品服务架构解密 丁鸣亮 网易考拉
作者讲解了网易考虑商品服务是如何从万级QPS改造为支撑百万QPS的，有以下几个重点：
1. 分离出商品的只读服务：只读服务只从缓存中读取数据，而由其他服务异步的方式更新缓存
2. 缓存：选择NKV（单一连接的方式，避免了Redis在加机器横向扩展时连接数不够用的问题）
3. 缓存数据更新方式：通过MQ的方式异步更新缓存
4. 服务和数据上分成两层：原子化数据和聚合化数据，在两个层面上都做缓存，避免一些业务需要从多个原子化数据服务中读取数据的问题。
5. 更清晰的数据模型：接口模型与存储模型分离

## 2020-02-13
#### 利用braft快速搭建高性能分布式系统 王耀 百度云架构师
braft是一个基于brpc实现的raft协议的C++版本，raft是当前最易于理解的强一致性算法，其中etcd就是基于raft协议实现一致性。几乎所有的分布式存储系统都要考虑到多副本数据一致性的问题，如：MySQL副本的强一致性实现，TiDB中TiKV的多副本一致性都是基于raft协议实现的。
braft在百度云的应用：
1. 元信息管理
    * 容器系统Master
    * 虚机系统Master
    * 流式计算系统Master
2. 存储系统
    * 强一致性MySQL
    * 分布式块存储CDS
    * 分布式文件系统CFS
    * 分布式NewSQL TafDB

#### 智能即时物流的分布式系统架构设计 宋斌 美团点评
**即时物流面临的技术问题和挑战：**
1. 供需匹配过程超大规模计算问题
2. 波峰波谷效应明显，流量高峰是平常10多倍
3. 故障容忍度极低，不能宕机，数据不能丢失，可用性要求高
4. 数据实时性、准确性要求高，对延迟、异常敏感

**服务架构的演进**
单服务架构->分层服务架构->微服务架构->多中心->单元化

**如何实现高可用高扩展性**
1. 尽可能把有状态应用修改为无状态应用，方便按需扩容机器
2. 基于Databus实现异步的方式更新缓存，实现最终一致性
3. 事前全链路压测、随机故障演练
4. 事中异常监控报警、快速故障定位
5. 事后通过扩容、限流熔断、降级，一键兜底容灾

## 2020-02-14
#### 多地域服务网格设计与多环境基础架构实践 雷飞尉 同程艺龙
作者讲解的服务网格与Kubernetes的ServiceMesh类似，通过部署守护进程的Sidecar的方式实现服务治理，作者实现的服务网格有以下特点：
    实现服务网格的跨地域的核心是实现服务注册发现的跨地域，实现方式如下：
    * 每个IDC部署自己的etcd服务注册中心
    * IDC之间的etcd服务注册中心进行双向同步数据，这样就能保证每个IDC机房的数据都是最新的，即时发生网络隔离也能保证高可用。
    * 服务发现时把idc作为服务名称的一个标识，如：ServiceA.BJ.idc1 ServiceA.SH.idc1。通过这种方式就能知道当前请求需要调用的是哪个IDC机房的服务。
    
**2. 泳道功能**
    作者说的泳道功能其实就是多环境部署，基于此可以实现全链路的灰度发布、ABtest。实现原理如下：
    * 在Nginx入口处表示给当前请求添加环境标识，如stage
    * 服务在启动注册当前实例时，标识当前实例所在的环境，如：prod、stage等。
    * 类似于APM中traceId的方式，把环境标识在所有服务调用中传递下去，如基于JavaAgent方式的Hook机制拦截所有远程调用如：rpc、redis调用等，并通过ThreadLocal的方式传递下去。
    * 在进行服务调用时，根据当前ThreadLocal中的环境标识调用对应环境的服务

#### 网易Spring Cloud万级实例调优与增强 朱剑峰 网易云
作者介绍了网易云架构的演进过程：
1. 优化重构注册中心
    网易云曾经使用的是Dubbo的rpc框架，在实例比较多的时候，ZK的网络传输会成为瓶颈。由于注册中心本质上是AP架构，而ZK是CP架构的，所以网易云重构了服务注册中心，转为了CP架构。
2. NSF微服务框架1.0
通过Java Agent的方式使非微服务项目可以很方便的接入到微服务中，Agent与服务注册中心和服务控制中心基于grpc通信，实现配置和一些参数的实时下发通知，由此实现了一些特色功能
 * 服务治理策略实时生效
 * 多环境支持，这里的多环境与上面同程艺龙架构师分享的泳道功能是一致的，这里叫做染色，即通过全链路传递环境标识的方式实现多环境运行。




